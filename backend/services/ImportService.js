const multer = require("multer");
const csv = require("csv-parser");
const unzipper = require("unzipper");
const { pool } = require("../db.js");
const { Readable } = require("stream");
const { initializeTables, tableExists } = require("../initTables.js");

class ImportService {
  constructor() {
    this.storage = multer.memoryStorage();
    this.upload = multer({
      storage: this.storage,
      fileFilter: (req, file, cb) => {
        if (!file.originalname.toLowerCase().endsWith(".zip")) {
          return cb(new Error("Only .zip files are allowed"));
        }
        cb(null, true);
      },
    });
    this.batchSize = 1000;
    this.tableOrder = [
      "agency",
      "calendar",
      "stops",
      "routes",
      "shapes",
      "trips",
      "stop_times",
    ];
  }

  async insertImportedData(userId, fileName) {
    try {
      console.log("üìù Importing metadata for user:", userId, "file:", fileName);
      const [result] = await pool.execute(
        "INSERT INTO projects (user_id, file_name, import_date) VALUES (?, ?, NOW())",
        [userId, fileName]
      );
      console.log("‚úÖ Metadata inserted - ID:", result.insertId);
      return result.insertId;
    } catch (error) {
      console.error("‚ùå Error inserting metadata:", error);
      throw error;
    }
  }

  async getTableColumns(tableName) {
    const [columns] = await pool.query(
      `
      SELECT COLUMN_NAME
      FROM information_schema.COLUMNS
      WHERE TABLE_SCHEMA = DATABASE()
      AND TABLE_NAME = ?
    `,
      [tableName]
    );
    return new Set(columns.map((col) => col.COLUMN_NAME));
  }

  async processBatch(tableName, batch) {
    if (batch.length === 0) return;

    if (!(await tableExists(tableName))) {
      console.error(`‚ùå Table ${tableName} does not exist and is not expected`);
      return;
    }

    const validColumns = await this.getTableColumns(tableName);
    const columns = Object.keys(batch[0]).filter((col) =>
      validColumns.has(col)
    );

    if (columns.length === 0) {
      console.log(`‚ö†Ô∏è No valid columns found for ${tableName}, skipping batch`);
      return;
    }

    const placeholders = columns.map(() => "?").join(",");
    const updatableColumns = columns.filter(
      (col) => col !== "user_id" && col !== "project_id"
    );
    let sql = `INSERT IGNORE INTO ${tableName} (${columns.join(",")}) VALUES ?`;
    if (updatableColumns.length > 0) {
      const updateClause = updatableColumns
        .map((col) => `${col} = VALUES(${col})`)
        .join(",");
      sql += ` ON DUPLICATE KEY UPDATE ${updateClause}`;
    }

    const values = batch.map((row) => columns.map((col) => row[col]));

    let retries = 3;
    while (retries > 0) {
      try {
        await pool.query(sql, [values]);
        break;
      } catch (error) {
        if (error.code === "ER_NO_REFERENCED_ROW_2") {
          const [fk] = await pool.query(
            `
            SELECT COLUMN_NAME, REFERENCED_TABLE_NAME, REFERENCED_COLUMN_NAME
            FROM information_schema.KEY_COLUMN_USAGE
            WHERE TABLE_SCHEMA = DATABASE()
            AND TABLE_NAME = ?
            AND REFERENCED_TABLE_NAME IS NOT NULL
            LIMIT 1
          `,
            [tableName]
          );
          if (fk) {
            const missingRefs = new Set();
            batch.forEach((row) => {
              if (
                row[fk.COLUMN_NAME] &&
                !missingRefs.has(row[fk.COLUMN_NAME])
              ) {
                missingRefs.add(row[fk.COLUMN_NAME]);
              }
            });
            const refValues = Array.from(missingRefs).map((ref) => [
              ref,
              batch[0].user_id,
              batch[0].project_id,
            ]);
            await pool.query(
              `INSERT IGNORE INTO ${fk.REFERENCED_TABLE_NAME} (${fk.REFERENCED_COLUMN_NAME}, user_id, project_id) VALUES ?`,
              [refValues]
            );
            await pool.query(sql, [values]);
          }
        } else if (error.code === "ECONNRESET" && retries > 0) {
          console.warn(
            `‚ö†Ô∏è Connection reset, retrying (${retries} attempts left)`
          );
          retries--;
          await new Promise((res) => setTimeout(res, 1000));
        } else {
          console.error(
            `‚ùå Batch import error for ${tableName}:`,
            error.message
          );
          throw error;
        }
      }
    }
  }

  async importGTFSData(req, res) {
    try {
      const userId = req.user.id;
      console.log("üì§ Import request received from user:", userId);

      if (!req.file) {
        console.log("‚ö†Ô∏è No file uploaded");
        return res.status(400).json({ message: "No file uploaded" });
      }

      console.log("üìÅ Processing file:", req.file.originalname);
      console.log("üì¶ Starting ZIP extraction...");

      const [userExists] = await pool.query(
        "SELECT COUNT(*) as count FROM users WHERE id = ?",
        [userId]
      );
      if (userExists[0].count === 0) {
        throw new Error(
          `User with id ${userId} does not exist in the users table`
        );
      }

      const files = {};
      await new Promise((resolve, reject) => {
        const zipStream = Readable.from(req.file.buffer).pipe(unzipper.Parse());
        let totalFiles = 0;
        let processedFiles = 0;

        zipStream
          .on("entry", (entry) => {
            const fileName = entry.path;
            const chunks = [];
            totalFiles++;

            entry
              .on("data", (chunk) => chunks.push(chunk))
              .on("end", () => {
                files[fileName] = Buffer.concat(chunks);
                console.log("üìÑ Extracted:", fileName);
                processedFiles++;
              })
              .on("error", (error) => reject(error));
          })
          .on("error", (error) => reject(error))
          .on("finish", () => {
            const waitForFiles = () => {
              if (processedFiles === totalFiles && totalFiles > 0) {
                console.log("‚úÖ All files extracted, total:", processedFiles);
                resolve();
              } else if (totalFiles === 0) {
                reject(new Error("No files found in ZIP"));
              } else {
                setTimeout(waitForFiles, 100);
              }
            };
            waitForFiles();
          });
      });

      console.log("‚úÖ ZIP extraction complete");

      const projectId = await this.insertImportedData(
        userId,
        req.file.originalname
      );
      console.log("üöÄ Starting file imports with projectId:", projectId);

      await initializeTables();

      for (const tableName of this.tableOrder) {
        const filename = `${tableName}.txt`;
        const buffer = files[filename];
        if (!buffer) {
          console.log(`‚ö†Ô∏è ${filename} not found in ZIP, skipping`);
          continue;
        }

        console.log(`üì• Importing ${filename} (${tableName})...`);

        let batch = [];
        await new Promise((resolve, reject) => {
          const stream = Readable.from(buffer).pipe(csv());
          stream
            .on("data", async (row) => {
              Object.keys(row).forEach((key) => {
                if (row[key] === "" || row[key] === undefined) {
                  row[key] = null;
                }
              });
              row.user_id = userId;
              row.project_id = projectId;

              batch.push(row);

              if (batch.length >= this.batchSize) {
                stream.pause();
                await this.processBatch(tableName, batch);
                batch = [];
                stream.resume();
              }
            })
            .on("end", async () => {
              if (batch.length > 0) {
                await this.processBatch(tableName, batch);
              }
              console.log(`‚úÖ Import completed for ${tableName}`);
              resolve();
            })
            .on("error", (error) => {
              console.error(
                `‚ùå CSV parsing error for ${tableName}:`,
                error.message
              );
              reject(error);
            });
        });
      }

      for (const [filename] of Object.entries(files)) {
        const tableName = filename.replace(".txt", "").toLowerCase();
        if (!this.tableOrder.includes(tableName)) {
          console.log(
            `‚ö†Ô∏è Skipping ${filename} as it is not a valid GTFS table`
          );
        }
      }

      console.log("üéâ Import process completed");
      return res.status(200).json({
        message: "GTFS data import completed",
        success: true,
        projectId,
      });
    } catch (error) {
      console.error("‚ùå GTFS Import Error:", error.message);
      return res.status(500).json({
        message: error.message || "Error importing GTFS data",
        success: false,
      });
    }
  }
}

module.exports = new ImportService();
